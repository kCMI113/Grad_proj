# Base settings for common model & experiment

experiment_name: "newdoca_241015"

# Model settings
model_name: "DOCA4Rec"

model_dataset:
  # GenDataset : generative image, DescriptionDataset : image description
  train_dataset: "TrainDataset"
  # TestGenDataset : generative image, TestDescriptionDataset : image description
  test_dataset: "TestDataset"

model_arguments:
  hidden_size: 512        # hidden size for multi head attention
  num_attention_heads: 4  # number of heads for multi head attention
  num_hidden_layers: 2    # number of transformer layers
  max_len: 90             # length of input sequence
  dropout_prob: 0.2       # dropout probability
  hidden_act: "gelu"      # Activation function for attention, mlp
  pos_emb: True           # True if using positional embedding
  mask_prob: 0.4          # Input sequence masking probability
  model_ckpt_path: "./model/0427000421/model_val_9.835206767016421.pt"

  num_mlp_layers: 3       # number of mlp layers
  merge: "concat"            # ways to merge output of bert4rec and image embeddings. One of ["concat", "mul"].

  std:  0.0               # noise std(available if std value is minus, the std is adjusting std )
  mean: 0.0               # noise mean

  loss: 'BPR'              # ["CE", "BPR"]
  HNS: True                # image based hard negative sampling, loss should be "BPR"
  n_HNS: 1                 # num of hard negative sample at each epoch, HNS should be True, range:(0,)
  n_NS : 1                 # num of negative sample at each epoch, loss should be "BPR", range:(0,)


lr: 0.0001
lr_step: 25


epoch: 200
batch_size: 64
weight_decay: 0.001 # weight decay for Adam
num_workers: 4
valid_step: 3 # number of steps to run validation

data_local: False       # True if loading dataset from local directory, else download from huggingface hub
data_repo: "sequential"
dataset: "small"
data_version: "278686a051240309cc4e55ba61990b48171e115a"

n_cuda: "0"
