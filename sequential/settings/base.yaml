# Base settings for common model & experiment

experiment_name: "cross attention"

# Model settings
model_name: "BERT4Rec"
model_arguments:
  hidden_size: 512       # hidden size for multi head attention
  num_attention_heads: 4  # number of heads for multi head attention
  num_encoder_layers: 2    # number of transformer layers
  num_decoder_layers: 2
  max_len: 90             # length of input sequence
  dropout_prob: 0.2       # dropout probability
  num_mlp_layers: 3       # number of mlp layers
  pos_emb: True           # True if using positional embedding
  img_noise: True         # Add noise into generated image
  std: 0.01               # noise std(available if img_noise is True)
  mean: 0                 # noise mean(available if img_noise is True)
  hidden_act: "gelu"      # Activation function for attention, mlp
  num_gen_img: 2          # Number of generated image for mlp input
  mask_prob: 0.4          # Input sequence masking probability
  description_group: True # group by same image description
  merge: "mul"            # ways to merge output of bert4rec and image embeddings. One of ["concat", "mul"].
  detail_text: True      # using embedding of detail_text of item

neg_size : 50             # number of sampling cadidate oiik
neg_sample_size : 5       # number of negative sample
neg_sampling : False      # True if use negative sampling

lr: 0.0001
epoch: 50
batch_size: 32
weight_decay: 0.001 # weight decay for Adam
num_workers: 4
valid_step: 1 # number of steps to run validation

data_local: False       # True if loading dataset from local directory, else download from huggingface hub
data_repo: "sequential"
dataset: "small"
data_version: "1886aac081a03f5dbf635f4d306195556f7cf4ba"

n_cuda: "0"
