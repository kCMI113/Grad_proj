{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U fashion-clip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "item_data = pd.read_csv(\"./data/articles.csv\")\n",
    "interaction_data = pd.read_csv(\"./data/transactions_train.csv\")\n",
    "user_data = pd.read_csv(\"./data/customers.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### img prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "def img_by_id(df, article_id:int, no_list:list, echo:int=1, img_show:bool=True):\n",
    "    if article_id in no_list:\n",
    "        return\n",
    "    if echo:\n",
    "        display(df[df.article_id == article_id])\n",
    "\n",
    "    img_id = \"0\"+str(article_id)\n",
    "    img = Image.open(\"./data/images/\"+img_id[0:3]+\"/\"+img_id+\".jpg\")\n",
    "\n",
    "    if img_show:\n",
    "        img.show()\n",
    "\n",
    "def find_no_img_item(df):\n",
    "    no_img = []\n",
    "\n",
    "    for item in tqdm(df.iterrows(), total=len(df)):\n",
    "        try:\n",
    "            img_by_id(df, item[1][0], no_list=no_img, echo=0, img_show=False)\n",
    "        except FileNotFoundError:\n",
    "            no_img.append(item[0])\n",
    "\n",
    "    return no_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105542/105542 [00:25<00:00, 4080.37it/s]\n"
     ]
    }
   ],
   "source": [
    "no_img_ids = find_no_img_item(item_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "442"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(no_img_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_img_article_id = [item_data.iloc[x].article_id for x in no_img_ids]\n",
    "\n",
    "n_item_data = item_data.drop(no_img_ids, axis=0).reset_index(drop=True)\n",
    "n_interaction_data = interaction_data[~interaction_data[\"article_id\"].isin(no_img_article_id)].reset_index(drop=True)\n",
    "\n",
    "user2idx = {v:k for k,v in enumerate(user_data['customer_id'].unique())}\n",
    "item2idx = {v:k for k,v in enumerate(n_item_data['article_id'].unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import alexnet, AlexNet_Weights, resnet18, ResNet18_Weights, vgg16, VGG16_Weights\n",
    "from fashion_clip.fashion_clip import FashionCLIP\n",
    "\n",
    "\n",
    "# # load pretrained alexnet\n",
    "# model_alex = alexnet(weights=AlexNet_Weights.IMAGENET1K_V1)\n",
    "# model_res = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "# model_vgg = vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# # del last clf layer\n",
    "# model_alex.classifier = model_alex.classifier[:-3]\n",
    "# model_res.fc = nn.Identity()\n",
    "# model_vgg.classifier = model_vgg.classifier[:-3]\n",
    "\n",
    "fclip = FashionCLIP('fashion-clip')\n",
    "\n",
    "images = [\"./data/images/\" + \"0\" + str(k)[0:2] + \"/\" + \"0\"+str(k) + \".jpg\" for k in n_item_data[\"article_id\"].tolist()]\n",
    "# image_embeddings = fclip.encode_images(images, batch_size=32)\n",
    "\n",
    "\n",
    "# feat_map_res = make_feature_map(model_res,n_item_data, book2idx)\n",
    "# feat_map_alex = make_feature_map(model_alex, n_book_data, book2idx)\n",
    "# feat_map_vgg = make_feature_map(model_vgg,n_book_data, book2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 만들어둔 임베딩 csv로 저장해두기\n",
    "# pd.DataFrame(images).to_csv(\"img_list.csv\", index=False)\n",
    "# pd.DataFrame(image_embeddings).to_csv(\"img_emb.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = pd.read_csv(\"img_list.csv\")\n",
    "img_emb = pd.read_csv(\"img_emb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([105100, 512])\n"
     ]
    }
   ],
   "source": [
    "print(img_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_emb = torch.tensor(img_emb.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9380, dtype=torch.float64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# img 16=\"0118458003\", 17=\"0118458004\"\n",
    "res = nn.functional.cosine_similarity(img_emb[16], img_emb[17], dim=0)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "class HMDataset(Dataset):\n",
    "    def __init__(self, df, user2idx, item2idx) -> None:\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.user2idx = user2idx\n",
    "        self.item2idx = item2idx\n",
    "        self.n_user = len(self.user2idx)\n",
    "        self.n_item = len(self.item2idx)\n",
    "        # mapping id2idx\n",
    "        self.df['article_id'] = self.df['article_id'].map(self.item2idx)\n",
    "        self.df['customer_id'] = self.df['customer_id'].map(self.user2idx)\n",
    "        self.df['neg'] = np.zeros(len(self.df), dtype=int)\n",
    "\n",
    "        self._make_triples_data()\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        user = self.df.customer_id[index]\n",
    "        pos = self.df.article_id[index]\n",
    "        neg = self.df.neg[index]\n",
    "        return user, pos, neg\n",
    "    \n",
    "    def _neg_sampling(self, pos_list):\n",
    "        neg = np.random.randint(0,self.n_item,1)\n",
    "        while neg in pos_list:\n",
    "            neg = np.random.randint(0,self.n_item,1)\n",
    "        return neg\n",
    "\n",
    "    def _make_triples_data(self):\n",
    "        for id in tqdm(range(self.n_user)):\n",
    "            pos_list = (self.df[self.df.customer_id==id].article_id).tolist()\n",
    "            for i in range(len(self.df[self.df.customer_id==id])):\n",
    "                idx = self.df[self.df['customer_id'] == id].index[i]\n",
    "                self.df.at[idx, 'neg'] = self._neg_sampling(pos_list)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 210/1371980 [01:30<376:41:46,  1.01it/s]"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "dataset = HMDataset(n_interaction_data, user2idx, item2idx)\n",
    "train_dataset, test_dataset = random_split(dataset, [0.8,0.2])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class VBPR(nn.Module):\n",
    "    def __init__(self, n_user, n_item, K, D, F, feature_map) -> None:\n",
    "        super().__init__()\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        self.K = K\n",
    "        self.D = D\n",
    "        self.F = F\n",
    "\n",
    "        self.offset = nn.Parameter(torch.zeros(1))\n",
    "        self.user_bias = nn.Embedding(self.n_user,1)\n",
    "        self.item_bias = nn.Embedding(self.n_item,1)\n",
    "        self.vis_bias = nn.Embedding(self.F,1)\n",
    "        self.user_emb = nn.Embedding(self.n_user,self.K)\n",
    "        self.item_emb = nn.Embedding(self.n_item,self.K)\n",
    "        self.img_vis_emb = nn.Embedding(self.D, self.F)\n",
    "        self.user_vis_emb = nn.Embedding(self.n_user, self.D)\n",
    "        self.feature_map = feature_map\n",
    "    \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _get_feature_map(self, itemset):\n",
    "        res = torch.tensor([])\n",
    "        for item in itemset:\n",
    "            res = torch.concat((res, self.feature_map[item.item()]), dim=0)\n",
    "        return res\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.user_bias.weight)\n",
    "        nn.init.xavier_uniform_(self.item_bias.weight.data)\n",
    "        nn.init.xavier_uniform_(self.vis_bias.weight.data)\n",
    "        nn.init.xavier_uniform_(self.user_emb.weight.data)\n",
    "        nn.init.xavier_uniform_(self.item_emb.weight.data)\n",
    "        nn.init.xavier_uniform_(self.img_vis_emb.weight.data)\n",
    "        nn.init.xavier_uniform_(self.user_vis_emb.weight.data)\n",
    "    \n",
    "    def cal_each(self, user, item):\n",
    "        feat_map = self._get_feature_map(item).T\n",
    "        vis_term = ((self.user_vis_emb(user))@(self.img_vis_emb.weight@(feat_map))).sum(dim=1) + (self.vis_bias.weight.T)@(feat_map)\n",
    "        mf_term = self.offset + self.user_bias(user).T + self.item_bias(item).T + (self.user_emb(user)@self.item_emb(item).T).sum(dim=1).unsqueeze(dim=0)\n",
    "        params = (self.offset, self.user_bias(user), self.item_bias(item), self.vis_bias.weight, self.user_emb(user), self.item_emb(item), self.img_vis_emb.weight, self.user_vis_emb(user))\n",
    "        return (mf_term+vis_term).squeeze(), params\n",
    "    \n",
    "    def forward(self, user, pos, neg):\n",
    "        xui, pos_params = self.cal_each(user,pos)\n",
    "        xuj, neg_params = self.cal_each(user,neg)\n",
    "        return (xui-xuj), pos_params, neg_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class VBPR(nn.Module):\n",
    "    def __init__(self, n_user, n_item, K, D, F, feature_map) -> None:\n",
    "        super().__init__()\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        self.K = K\n",
    "        self.D = D\n",
    "        self.F = F\n",
    "\n",
    "        self.offset = nn.Parameter(torch.zeros(1))\n",
    "        self.user_bias = nn.Embedding(self.n_user,1)\n",
    "        self.item_bias = nn.Embedding(self.n_item,1)\n",
    "        self.vis_bias = nn.Embedding(self.F,1)\n",
    "        self.user_emb = nn.Embedding(self.n_user,self.K)\n",
    "        self.item_emb = nn.Embedding(self.n_item,self.K)\n",
    "        self.img_vis_emb = nn.Embedding(self.D, self.F)\n",
    "        self.user_vis_emb = nn.Embedding(self.n_user, self.D)\n",
    "        self.feature_map = feature_map\n",
    "    \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _get_feature_map(self, itemset):\n",
    "        res = torch.tensor([])\n",
    "        for item in itemset:\n",
    "            res = torch.concat((res, self.feature_map[item.item()]), dim=0)\n",
    "        return res\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.user_bias.weight)\n",
    "        nn.init.xavier_uniform_(self.item_bias.weight.data)\n",
    "        nn.init.xavier_uniform_(self.vis_bias.weight.data)\n",
    "        nn.init.xavier_uniform_(self.user_emb.weight.data)\n",
    "        nn.init.xavier_uniform_(self.item_emb.weight.data)\n",
    "        nn.init.xavier_uniform_(self.img_vis_emb.weight.data)\n",
    "        nn.init.xavier_uniform_(self.user_vis_emb.weight.data)\n",
    "    \n",
    "    def cal_each(self, user, item):\n",
    "        feat_map = self._get_feature_map(item).T\n",
    "        vis_term = ((self.user_vis_emb(user))@(self.img_vis_emb.weight@(feat_map))).sum(dim=1) + (self.vis_bias.weight.T)@(feat_map)\n",
    "        mf_term = self.offset + self.user_bias(user).T + self.item_bias(item).T + (self.user_emb(user)@self.item_emb(item).T).sum(dim=1).unsqueeze(dim=0)\n",
    "        params = (self.offset, self.user_bias(user), self.item_bias(item), self.vis_bias.weight, self.user_emb(user), self.item_emb(item), self.img_vis_emb.weight, self.user_vis_emb(user))\n",
    "        return (mf_term+vis_term).squeeze(), params\n",
    "    \n",
    "    def forward(self, user, pos, neg):\n",
    "        xui, pos_params = self.cal_each(user,pos)\n",
    "        xuj, neg_params = self.cal_each(user,neg)\n",
    "        return (xui-xuj), pos_params, neg_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPRLoss(nn.Module):\n",
    "    def __init__(self, reg_theta, reg_beta, reg_e) -> None:\n",
    "        super().__init__()\n",
    "        self.reg_theta = reg_theta\n",
    "        self.reg_beta = reg_beta\n",
    "        self.reg_e = reg_e\n",
    "    \n",
    "    def _cal_l2(self, *tensors):\n",
    "        total = 0\n",
    "        for tensor in tensors:\n",
    "            total += tensor.pow(2).sum()\n",
    "        return 0.5 * total\n",
    "\n",
    "    def _reg_term(self, pos_params, neg_params):\n",
    "        alpha, beta_u, beta_pos, beta_prime_pos, gamma_u, gamma_pos, e_pos, theta_u = pos_params\n",
    "        _, _, beta_neg, beta_prime_neg, _, gamma_neg, e_neg, _ = neg_params\n",
    "\n",
    "        reg_out = self.reg_theta * self._cal_l2(alpha, beta_u, beta_pos, beta_neg, theta_u, gamma_u, gamma_pos, gamma_neg)\n",
    "        reg_out += self.reg_beta * self._cal_l2(beta_prime_pos, beta_prime_neg)\n",
    "        reg_out += self.reg_e * self._cal_l2(e_pos, e_neg)\n",
    "\n",
    "        return reg_out\n",
    "\n",
    "    def forward(self, diff, pos_params, neg_params):\n",
    "        loss = -nn.functional.logsigmoid(diff).sum()\n",
    "        loss += self._reg_term(pos_params, neg_params)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def train(model, optimizer, dataloader, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for user, pos, neg in tqdm(dataloader):\n",
    "        user = user.to(device)\n",
    "        pos = pos.to(device)\n",
    "        neg = neg.to(device)\n",
    "\n",
    "        diff, pos_params, neg_params = model(user, pos, neg)\n",
    "        loss = criterion(diff, pos_params, neg_params)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss/len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "n_user = dataset.n_user\n",
    "n_item = dataset.n_item\n",
    "\n",
    "K = 20\n",
    "D = 20\n",
    "F = 4096\n",
    "F_res = 512\n",
    "\n",
    "\n",
    "reg_theta = 0.1\n",
    "reg_beta = 0.1\n",
    "reg_e = 0\n",
    "\n",
    "lr = 0.001\n",
    "epoch = 20\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
    "criterion = BPRLoss(reg_theta, reg_beta, reg_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_map = feat_map_alex\n",
    "\n",
    "vbpr_alex = VBPR(n_user, n_item, K, D, F, feat_map)\n",
    "optimizer = Adam(params = vbpr_alex.parameters(), lr=lr)\n",
    "alex_train_loss = []\n",
    "\n",
    "for i in range(epoch):\n",
    "    alex_train_loss.append(train(vbpr_alex, optimizer, train_dataloader, criterion, device))\n",
    "    print(f'EPOCH : {i} | LOSS : {alex_train_loss[-1]:.10}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_res = 512\n",
    "feat_map = feat_map_res\n",
    "\n",
    "vbpr_res = VBPR(n_user, n_item, K, D, F_res, feat_map)\n",
    "optimizer = Adam(params = vbpr_res.parameters(), lr=lr)\n",
    "res_train_loss = []\n",
    "\n",
    "for i in range(epoch):\n",
    "    res_train_loss.append(train(vbpr_res, optimizer, train_dataloader, criterion, device))\n",
    "    print(f'EPOCH : {i} | LOSS : {res_train_loss[-1]:.10}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_map = feat_map_vgg\n",
    "\n",
    "vbpr_vgg = VBPR(n_user, n_item, K, D, F, feat_map)\n",
    "optimizer = Adam(params = vbpr_vgg.parameters(), lr=lr)\n",
    "vgg_train_loss = []\n",
    "\n",
    "for i in range(epoch):\n",
    "    vgg_train_loss.append(train(vbpr_vgg, optimizer, train_dataloader, criterion, device))\n",
    "    print(f'EPOCH : {i} | LOSS : {vgg_train_loss[-1]:.10}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(epoch), alex_train_loss, label=\"Alexnet\")\n",
    "plt.plot(range(epoch), res_train_loss, label=\"Resnet\")\n",
    "plt.plot(range(epoch), vgg_train_loss, label=\"VGG\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### change n_factor(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 40\n",
    "# D = 20\n",
    "\n",
    "# reg_theta = 0.1\n",
    "# reg_beta = 0.1\n",
    "# reg_e = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vbpr_alex = VBPR(n_user, n_item, K, D, F, feat_map_alex)\n",
    "optimizer = Adam(params = vbpr_alex.parameters(), lr=lr)\n",
    "alex_train_loss_40 = []\n",
    "\n",
    "for i in range(epoch):\n",
    "    alex_train_loss_40.append(train(vbpr_alex, optimizer, train_dataloader, criterion, device))\n",
    "    print(f'EPOCH : {i} | LOSS : {alex_train_loss_40[-1]:.10}')\n",
    "\n",
    "print(\"--------------------------------------------------------------------\")\n",
    "\n",
    "vbpr_res = VBPR(n_user, n_item, K, D, F_res, feat_map_res)\n",
    "optimizer = Adam(params = vbpr_res.parameters(), lr=lr)\n",
    "res_train_loss_40 = []\n",
    "\n",
    "for i in range(epoch):\n",
    "    res_train_loss_40.append(train(vbpr_res, optimizer, train_dataloader, criterion, device))\n",
    "    print(f'EPOCH : {i} | LOSS : {res_train_loss_40[-1]:.10}')\n",
    "\n",
    "print(\"--------------------------------------------------------------------\")\n",
    "\n",
    "vbpr_vgg = VBPR(n_user, n_item, K, D, F, feat_map_vgg)\n",
    "optimizer = Adam(params = vbpr_vgg.parameters(), lr=lr)\n",
    "vgg_train_loss_40 = []\n",
    "\n",
    "for i in range(epoch):\n",
    "    vgg_train_loss_40.append(train(vbpr_vgg, optimizer, train_dataloader, criterion, device))\n",
    "    print(f'EPOCH : {i} | LOSS : {vgg_train_loss_40[-1]:.10}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top K rec test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recommender:\n",
    "    def __init__(self, model, query_img, train_dataset, n_item, feature_map, device) -> None:\n",
    "        self.model = model\n",
    "        self.train_df = train_dataset.dataset.df\n",
    "        self.all_item = set(range(0,n_item))\n",
    "        self.query_img = query_img\n",
    "        self.feature_map = feature_map\n",
    "        self.device = device\n",
    "\n",
    "    def _get_img_sim(self, itemset:list):\n",
    "        print(\"GET IMG SIM\")\n",
    "        res = []\n",
    "        for item in itemset:\n",
    "            res.append(nn.functional.cosine_similarity(self.query_img, self.feature_map[item.item()]))\n",
    "        return res\n",
    "\n",
    "    def _get_unobs_items(self, user_idx):\n",
    "        obs_item_set = set(self.train_df[self.train_df.user_id==user_idx].isbn)\n",
    "        return list(self.all_item - obs_item_set)\n",
    "\n",
    "    def user_rank(self, user_idx:int, top_k:int=None):\n",
    "        self.model.eval()\n",
    "        unobs_itemset = self._get_unobs_items(user_idx)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            itemset = torch.tensor(unobs_itemset).to(self.device)\n",
    "            user = torch.tensor(np.full(len(itemset), user_idx)).to(self.device)\n",
    "            img_sim = torch.tensor(self._get_img_sim(itemset))\n",
    "\n",
    "            out, _ = self.model.cal_each(user, itemset)\n",
    "            out = out + img_sim\n",
    "            scores = np.array(torch.concat((user.unsqueeze(dim=1),itemset.unsqueeze(dim=1),out.unsqueeze(dim=1)), dim=1))\n",
    "       \n",
    "        sorted_scores = scores[(-scores[:, 2]).argsort()]\n",
    "        return sorted_scores[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(recommender, test_dataset):\n",
    "    df = test_dataset.dataset.df\n",
    "    user_list = df['user_id'].unique()\n",
    "    res_true = {}\n",
    "    res_topk = {}\n",
    "    res_hit = {}\n",
    "    \n",
    "    for user in tqdm(user_list[:50]):\n",
    "        true_item = df[df.user_id==user].isbn\n",
    "        if len(true_item)>4:\n",
    "            res = recommender.user_rank(user, 20)\n",
    "            topk = res[:,1]\n",
    "            hit = len(set(true_item).intersection(set(topk)))\n",
    "            res_true[user] = list(true_item)\n",
    "            res_topk[user] = list(topk)\n",
    "            res_hit[user] = hit\n",
    "    \n",
    "    return res_true, res_topk, res_hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_isbn = \"0440234743\"\n",
    "img = prepare_img(query_isbn)\n",
    "query = model_res(img)\n",
    "# res = img_sim(query, feat_map_vgg)\n",
    "# res\n",
    "recommender = Recommender(vbpr_res, query, train_dataset, n_item, feat_map_res, device)\n",
    "\n",
    "res_true, res_topk, res_hit = eval(recommender, test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sequential",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
