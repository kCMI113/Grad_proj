{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U fashion-clip\n",
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 캐글 계정 -> 셋팅 -> 토큰생성 -> kaggle.json 파일 다운로드 됨\n",
    "# root/.kaggle 폴더 생성 후, kaggle.json 파일을 .kaggle 폴더로 이동\n",
    "# kaggle.json 파일 내부에 적혀있는 username과 key를 아래 코드에 넣어 설정 후 H&M 데이터 압축파일 다운로드\n",
    "\n",
    "os.environ[\"KAGGLE_USERNSME\"] = \"유저 이름\"\n",
    "os.environ[\"KAGGLE_KEY\"] = \"유저 키\"\n",
    "\n",
    "!kaggle competitions download -c h-and-m-personalized-fashion-recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data # 데이터 폴더 생성\n",
    "!pwd # 현재 경로 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원하는 경로에 압축 풀고 싶다면 -d 이후에 원하는 경로로 변경\n",
    "!unzip -q h-and-m-personalized-fashion-recommendations.zip -d ./data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "item_data = pd.read_csv(\"./data/articles.csv\")\n",
    "interaction_data = pd.read_csv(\"./data/transactions_train.csv\")\n",
    "# user_data = pd.read_csv(\"./data/customers.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### img prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "def img_by_id(df, article_id:int, no_list:list, echo:int=1, img_show:bool=True):\n",
    "    '''\n",
    "    article_id를 입력으로 받아 결과를 출력하는 함수\n",
    "    echo==1, 해당 아이템의 df row 출력\n",
    "    img_show=True, 해당 아이템의 이미지 출력\n",
    "    '''\n",
    "    if article_id in no_list:\n",
    "        return\n",
    "    if echo:\n",
    "        display(df[df.article_id == article_id])\n",
    "\n",
    "    img_id = \"0\"+str(article_id)\n",
    "    img = Image.open(\"./data/images/\"+img_id[0:3]+\"/\"+img_id+\".jpg\")\n",
    "\n",
    "    if img_show:\n",
    "        img.show()\n",
    "\n",
    "def find_no_img_item(df):\n",
    "    '''\n",
    "    이미지가 없는 아이템 찾아내는 함수\n",
    "    '''\n",
    "    no_img = []\n",
    "\n",
    "    for item in tqdm(df.iterrows(), total=len(df)):\n",
    "        try:\n",
    "            img_by_id(df, item[1][0], no_list=no_img, echo=0, img_show=False)\n",
    "        except FileNotFoundError:\n",
    "            no_img.append(item[0])\n",
    "\n",
    "    return no_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_img_ids = find_no_img_item(item_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(no_img_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지가 없는 아이템을 구매 데이터와 아이템 데이터에서 삭제\n",
    "no_img_article_id = [item_data.iloc[x].article_id for x in no_img_ids]\n",
    "n_item_data = item_data.drop(no_img_ids, axis=0).reset_index(drop=True)\n",
    "n_interaction_data = interaction_data[~interaction_data[\"article_id\"].isin(no_img_article_id)].reset_index(drop=True)\n",
    "\n",
    "# train/test split을 위해 이력이 3초과인 유저만 남김\n",
    "n_interaction_data = n_interaction_data.groupby('customer_id').filter(lambda x: len(x) > 3).reset_index(drop=True)\n",
    "\n",
    "# (user/item)id를 index 맵핑\n",
    "user2idx = {v:k for k,v in enumerate(n_interaction_data['customer_id'].unique())}\n",
    "item2idx = {v:k for k,v in enumerate(n_item_data['article_id'].unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(item2idx) # 전체 아이템 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision.models import alexnet, AlexNet_Weights, resnet18, ResNet18_Weights, vgg16, VGG16_Weights\n",
    "from fashion_clip.fashion_clip import FashionCLIP\n",
    "\n",
    "\n",
    "# # load pretrained alexnet\n",
    "# model_alex = alexnet(weights=AlexNet_Weights.IMAGENET1K_V1)\n",
    "# model_res = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "# model_vgg = vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# # del last clf layer\n",
    "# model_alex.classifier = model_alex.classifier[:-3]\n",
    "# model_res.fc = nn.Identity()\n",
    "# model_vgg.classifier = model_vgg.classifier[:-3]\n",
    "\n",
    "fclip = FashionCLIP('fashion-clip')\n",
    "\n",
    "# images = [\"./data/images/\" + \"0\" + str(k)[0:2] + \"/\" + \"0\"+str(k) + \".jpg\" for k in n_item_data[\"article_id\"].tolist()]\n",
    "# 패션 clip을 통한 이미지 임베딩 얻기\n",
    "# image_embeddings = fclip.encode_images(images, batch_size=32) \n",
    "# img_emb = torch.tensor(image_embeddings)\n",
    "\n",
    "\n",
    "\n",
    "# feat_map_res = make_feature_map(model_res,n_item_data, book2idx)\n",
    "# feat_map_alex = make_feature_map(model_alex, n_book_data, book2idx)\n",
    "# feat_map_vgg = make_feature_map(model_vgg,n_book_data, book2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 만들어둔 임베딩 csv로 저장해두기\n",
    "# pd.DataFrame(images).to_csv(\"img_list.csv\", index=False)\n",
    "# pd.DataFrame(image_embeddings).to_csv(\"img_emb.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 임베딩 csv를 사용하는 경우\n",
    "# img_list = pd.read_csv(\"img_list.csv\")\n",
    "img_emb = pd.read_csv(\"./data/img_emb.csv\")\n",
    "img_emb = torch.tensor(img_emb.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([105100, 512])\n"
     ]
    }
   ],
   "source": [
    "print(img_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9380, dtype=torch.float64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# img 16=\"0118458003\", 17=\"0118458004\"\n",
    "# 임베딩 잘 되었는지 확인하기\n",
    "res = nn.functional.cosine_similarity(img_emb[16], img_emb[17], dim=0)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "class HMDataset(Dataset):\n",
    "    def __init__(self, df, user2idx, item2idx, is_train:bool=True) -> None:\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.is_train = is_train\n",
    "        self.user2idx = user2idx\n",
    "        self.item2idx = item2idx\n",
    "        self.n_user = len(self.user2idx)\n",
    "        self.n_item = len(self.item2idx)\n",
    "        # mapping id2idx\n",
    "        self.df['article_id'] = self.df['article_id'].map(self.item2idx)\n",
    "        self.df['customer_id'] = self.df['customer_id'].map(self.user2idx)\n",
    "        \n",
    "        # train 데이터인 경우에만 neg 아이템이 생성\n",
    "        if is_train:\n",
    "            self.df['neg'] = np.zeros(len(self.df), dtype=int)\n",
    "            self._make_triples_data()\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        user = self.df.customer_id[index]\n",
    "        pos = self.df.article_id[index]\n",
    "        \n",
    "        if self.is_train:\n",
    "            neg = self.df.neg[index]\n",
    "            return user, pos, neg\n",
    "        \n",
    "        return user, pos\n",
    "    \n",
    "    def _neg_sampling(self, pos_list):\n",
    "        '''\n",
    "        사용된 아이템 리스트(pos_list)에 없는 아이템 하나를 negative sample로 추출\n",
    "        '''\n",
    "        neg = np.random.randint(0,self.n_item,1) \n",
    "        while neg in pos_list:\n",
    "            neg = np.random.randint(0,self.n_item,1) \n",
    "        return neg\n",
    "\n",
    "    def _make_triples_data(self):\n",
    "        for id in tqdm(range(self.n_user)):\n",
    "            user_df = self.df[self.df.customer_id==id] # 유저 한 명 선택 \n",
    "            pos_list = (user_df.article_id).tolist()   # 해당 유저가 사용한 아이템 모두 추출\n",
    "            for i in range(len(user_df)): # 유저의 모든 구매 이력에 neg sample을 추가해줌\n",
    "                idx = user_df.index[i] \n",
    "                self.df.at[idx, 'neg'] = self._neg_sampling(pos_list)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = n_interaction_data.groupby('customer_id').nth(-1) # 가장 마지막 구매 이력만 추출\n",
    "train_df = n_interaction_data[~n_interaction_data.index.isin(test_df.index)] # test에 해당하지 않는 데이터 모두 추출\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "train_df = train_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_interaction_data.shape, test_df.shape, train_df.shape, test_df.shape[0]+train_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split 잘 되었는지 확인해보기\n",
    "test_df.iloc[0].customer_id\n",
    "display(test_df[test_df.customer_id == '0021da829b898f82269fc51feded4eac2129058ee95bd75bb1591e2eb14ecc79'])\n",
    "display(train_df[train_df.customer_id == '0021da829b898f82269fc51feded4eac2129058ee95bd75bb1591e2eb14ecc79'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = HMDataset(train_df, user2idx, item2idx)\n",
    "# test_dataset = HMDataset(test_df, user2idx, item2idx, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_pt(data, path):\n",
    "#     with open(path, \"wb\") as file:\n",
    "#         torch.save(data, file)\n",
    "\n",
    "# save_pt(train_dataset, \"./dataset/train_dataset.pt\")\n",
    "# save_pt(test_dataset, \"./dataset/test_dataset.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "train_dataset = torch.load(\"./dataset/train_dataset.pt\")\n",
    "test_dataset = torch.load(\"./dataset/test_dataset.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_dat</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>price</th>\n",
       "      <th>sales_channel_id</th>\n",
       "      <th>neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>0</td>\n",
       "      <td>39766</td>\n",
       "      <td>0.050831</td>\n",
       "      <td>2</td>\n",
       "      <td>35869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>0</td>\n",
       "      <td>10398</td>\n",
       "      <td>0.030492</td>\n",
       "      <td>2</td>\n",
       "      <td>58328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>1</td>\n",
       "      <td>6314</td>\n",
       "      <td>0.015237</td>\n",
       "      <td>2</td>\n",
       "      <td>83245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>1</td>\n",
       "      <td>45887</td>\n",
       "      <td>0.016932</td>\n",
       "      <td>2</td>\n",
       "      <td>52650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>1</td>\n",
       "      <td>45888</td>\n",
       "      <td>0.016932</td>\n",
       "      <td>2</td>\n",
       "      <td>17427</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        t_dat  customer_id  article_id     price  sales_channel_id    neg\n",
       "0  2018-09-20            0       39766  0.050831                 2  35869\n",
       "1  2018-09-20            0       10398  0.030492                 2  58328\n",
       "2  2018-09-20            1        6314  0.015237                 2  83245\n",
       "3  2018-09-20            1       45887  0.016932                 2  52650\n",
       "4  2018-09-20            1       45888  0.016932                 2  17427"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_dat</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>price</th>\n",
       "      <th>sales_channel_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>8</td>\n",
       "      <td>29020</td>\n",
       "      <td>0.019814</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>12</td>\n",
       "      <td>26169</td>\n",
       "      <td>0.033881</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>26</td>\n",
       "      <td>20712</td>\n",
       "      <td>0.030492</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>83</td>\n",
       "      <td>25774</td>\n",
       "      <td>0.005068</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>113</td>\n",
       "      <td>40814</td>\n",
       "      <td>0.013542</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        t_dat  customer_id  article_id     price  sales_channel_id\n",
       "0  2018-09-20            8       29020  0.019814                 1\n",
       "1  2018-09-20           12       26169  0.033881                 2\n",
       "2  2018-09-20           26       20712  0.030492                 1\n",
       "3  2018-09-20           83       25774  0.005068                 2\n",
       "4  2018-09-20          113       40814  0.013542                 2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class VBPR(nn.Module):\n",
    "    def __init__(self, n_user, n_item, K, D, img_embedding) -> None:\n",
    "        super().__init__()\n",
    "        self.feat_map= img_embedding.float() # user * 512\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        self.K = K\n",
    "        self.D = D\n",
    "        self.F = self.feat_map.shape[1] \n",
    "\n",
    "        self.offset = nn.Parameter(torch.zeros(1))\n",
    "        self.user_bias = nn.Embedding(self.n_user,1) # user*1\n",
    "        self.item_bias = nn.Embedding(self.n_item,1) # item*1\n",
    "        self.vis_bias = nn.Embedding(self.F,1)       # 512*1\n",
    "        self.user_emb = nn.Embedding(self.n_user,self.K) # user*K\n",
    "        self.item_emb = nn.Embedding(self.n_item,self.K) # item*K\n",
    "        self.item_vis_emb = nn.Embedding(self.D, self.F) # D*K\n",
    "        self.user_vis_emb = nn.Embedding(self.n_user, self.D) # user*D\n",
    "    \n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.user_bias.weight)\n",
    "        nn.init.xavier_uniform_(self.item_bias.weight.data)\n",
    "        nn.init.xavier_uniform_(self.vis_bias.weight.data)\n",
    "        nn.init.xavier_uniform_(self.user_emb.weight.data)\n",
    "        nn.init.xavier_uniform_(self.item_emb.weight.data)\n",
    "        nn.init.xavier_uniform_(self.item_vis_emb.weight.data)\n",
    "        nn.init.xavier_uniform_(self.user_vis_emb.weight.data)\n",
    "    \n",
    "    def cal_each(self, user, item):\n",
    "        vis_term = (self.user_vis_emb(user)@(self.item_vis_emb.weight@(self.feat_map[item].T))).sum(dim=1) + (self.vis_bias.weight.T)@(self.feat_map[item].T)\n",
    "        mf_term = self.offset + self.user_bias(user).T + self.item_bias(item).T + (self.user_emb(user)@self.item_emb(item).T).sum(dim=1).unsqueeze(dim=0)\n",
    "        params = (self.offset, self.user_bias(user), self.item_bias(item), self.vis_bias.weight, self.user_emb(user), self.item_emb(item), self.item_vis_emb.weight, self.user_vis_emb(user))\n",
    "        return (mf_term+vis_term).squeeze(), params\n",
    "    \n",
    "    def forward(self, user, pos, neg):\n",
    "        xui, pos_params = self.cal_each(user,pos)\n",
    "        xuj, neg_params = self.cal_each(user,neg)\n",
    "        return (xui-xuj), pos_params, neg_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPRLoss(nn.Module):\n",
    "    def __init__(self, reg_theta, reg_beta, reg_e) -> None:\n",
    "        super().__init__()\n",
    "        self.reg_theta = reg_theta\n",
    "        self.reg_beta = reg_beta\n",
    "        self.reg_e = reg_e\n",
    "\n",
    "    \n",
    "    def _cal_l2(self, *tensors):\n",
    "        total = 0\n",
    "        for tensor in tensors:\n",
    "            total += tensor.pow(2).sum()\n",
    "        return 0.5 * total\n",
    "\n",
    "    def _reg_term(self, pos_params, neg_params):\n",
    "        alpha, beta_u, beta_pos, beta_prime_pos, gamma_u, gamma_pos, e_pos, theta_u = pos_params\n",
    "        _, _, beta_neg, beta_prime_neg, _, gamma_neg, e_neg, _ = neg_params\n",
    "\n",
    "        reg_out = self.reg_theta * self._cal_l2(alpha, beta_u, beta_pos, beta_neg, theta_u, gamma_u, gamma_pos, gamma_neg)\n",
    "        reg_out += self.reg_beta * self._cal_l2(beta_prime_pos, beta_prime_neg)\n",
    "        reg_out += self.reg_e * self._cal_l2(e_pos, e_neg)\n",
    "\n",
    "        return reg_out\n",
    "\n",
    "    def forward(self, diff, pos_params, neg_params):\n",
    "        loss = -nn.functional.logsigmoid(diff).sum() # sigma(x_uij)\n",
    "        loss += self._reg_term(pos_params, neg_params) # reg_term\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, dataloader, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for user, pos, neg in tqdm(dataloader):\n",
    "        user = user.to(device)\n",
    "        pos = pos.to(device)\n",
    "        neg = neg.to(device)\n",
    "\n",
    "        diff, pos_params, neg_params = model(user, pos, neg)\n",
    "        loss = criterion(diff, pos_params, neg_params)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss/len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "n_user = train_dataset.n_user\n",
    "n_item = train_dataset.n_item\n",
    "\n",
    "K = 20\n",
    "D = 20\n",
    "reg_theta = 0.1\n",
    "reg_beta = 0.1\n",
    "reg_e = 0\n",
    "\n",
    "lr = 0.001\n",
    "epoch = 10\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
    "criterion = BPRLoss(reg_theta, reg_beta, reg_e).to(device)\n",
    "img_emb = img_emb.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117072/117072 [31:49<00:00, 61.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH : 0 | LOSS : 21.55792761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117072/117072 [31:21<00:00, 62.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH : 1 | LOSS : 14.8724304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117072/117072 [31:22<00:00, 62.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH : 2 | LOSS : 15.57909412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117072/117072 [31:21<00:00, 62.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH : 3 | LOSS : 14.64994436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117072/117072 [31:13<00:00, 62.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH : 4 | LOSS : 15.53052269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117072/117072 [31:04<00:00, 62.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH : 5 | LOSS : 14.59987981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117072/117072 [31:05<00:00, 62.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH : 6 | LOSS : 15.46814419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117072/117072 [31:10<00:00, 62.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH : 7 | LOSS : 14.50447961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117072/117072 [31:11<00:00, 62.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH : 8 | LOSS : 15.48946428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117072/117072 [31:12<00:00, 62.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH : 9 | LOSS : 14.52219915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vbpr_20 = VBPR(n_user, n_item, K, D, img_emb).to(device)\n",
    "optimizer = Adam(params = vbpr_20.parameters(), lr=lr)\n",
    "train_loss_20 = []\n",
    "\n",
    "for i in range(epoch):\n",
    "    train_loss_20.append(train(vbpr_20, optimizer, train_dataloader, criterion, device))\n",
    "    print(f'EPOCH : {i} | LOSS : {train_loss_20[-1]:.10}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/vbpr/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 117072/117072 [37:02<00:00, 52.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH : 0 | LOSS : 23.03348557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117072/117072 [36:28<00:00, 53.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH : 1 | LOSS : 16.71129583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117072/117072 [35:59<00:00, 54.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH : 2 | LOSS : 17.20827946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117072/117072 [36:00<00:00, 54.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH : 3 | LOSS : 16.34205082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117072/117072 [36:03<00:00, 54.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH : 4 | LOSS : 17.28841537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117072/117072 [35:59<00:00, 54.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH : 5 | LOSS : 16.38602991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117072/117072 [36:00<00:00, 54.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH : 6 | LOSS : 17.21514433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117072/117072 [35:56<00:00, 54.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH : 7 | LOSS : 16.32535571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 55328/117072 [16:59<18:57, 54.29it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m train_loss_40 \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epoch):\n\u001b[0;32m----> 8\u001b[0m     train_loss_40\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvbpr_40\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEPOCH : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | LOSS : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss_40[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.10\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, dataloader, criterion, device)\u001b[0m\n\u001b[1;32m      7\u001b[0m pos \u001b[38;5;241m=\u001b[39m pos\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m neg \u001b[38;5;241m=\u001b[39m neg\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 10\u001b[0m diff, pos_params, neg_params \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(diff, pos_params, neg_params)\n\u001b[1;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/opt/conda/envs/vbpr/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/vbpr/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 40\u001b[0m, in \u001b[0;36mVBPR.forward\u001b[0;34m(self, user, pos, neg)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, user, pos, neg):\n\u001b[0;32m---> 40\u001b[0m     xui, pos_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcal_each\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     xuj, neg_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcal_each(user,neg)\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (xui\u001b[38;5;241m-\u001b[39mxuj), pos_params, neg_params\n",
      "Cell \u001b[0;32mIn[8], line 35\u001b[0m, in \u001b[0;36mVBPR.cal_each\u001b[0;34m(self, user, item)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcal_each\u001b[39m(\u001b[38;5;28mself\u001b[39m, user, item):\n\u001b[1;32m     34\u001b[0m     vis_term \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_vis_emb(user)\u001b[38;5;241m@\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_vis_emb\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m@\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeat_map[item]\u001b[38;5;241m.\u001b[39mT)))\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvis_bias\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mT)\u001b[38;5;241m@\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeat_map[item]\u001b[38;5;241m.\u001b[39mT)\n\u001b[0;32m---> 35\u001b[0m     mf_term \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffset \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_bias(user)\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_bias(item)\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m+\u001b[39m \u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;129;43m@self\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     36\u001b[0m     params \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffset, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_bias(user), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_bias(item), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvis_bias\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_emb(user), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_emb(item), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_vis_emb\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_vis_emb(user))\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (mf_term\u001b[38;5;241m+\u001b[39mvis_term)\u001b[38;5;241m.\u001b[39msqueeze(), params\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "K = 40\n",
    "\n",
    "vbpr_40 = VBPR(n_user, n_item, K, D, img_emb).to(device)\n",
    "optimizer = Adam(params = vbpr_40.parameters(), lr=lr)\n",
    "train_loss_40 = []\n",
    "\n",
    "for i in range(epoch):\n",
    "    train_loss_40.append(train(vbpr_40, optimizer, train_dataloader, criterion, device))\n",
    "    print(f'EPOCH : {i} | LOSS : {train_loss_40[-1]:.10}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117072/117072 [42:00<00:00, 46.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH : 0 | LOSS : 24.46191098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117072/117072 [41:52<00:00, 46.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH : 1 | LOSS : 18.60020132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117072/117072 [41:48<00:00, 46.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH : 2 | LOSS : 19.03125653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117072/117072 [41:50<00:00, 46.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH : 3 | LOSS : 18.29452593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117072/117072 [41:55<00:00, 46.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH : 4 | LOSS : 19.00511913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 48565/117072 [17:19<24:18, 46.98it/s]"
     ]
    }
   ],
   "source": [
    "K = 60\n",
    "\n",
    "vbpr_60 = VBPR(n_user, n_item, K, D, img_emb).to(device)\n",
    "optimizer = Adam(params = vbpr_60.parameters(), lr=lr)\n",
    "train_loss_60 = []\n",
    "\n",
    "for i in range(epoch):\n",
    "    train_loss_60.append(train(vbpr_60, optimizer, train_dataloader, criterion, device))\n",
    "    print(f'EPOCH : {i} | LOSS : {train_loss_60[-1]:.10}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(epoch), train_loss_20, label=\"VBPR(factor 20)\")\n",
    "plt.plot(range(epoch), train_loss_40, label=\"VBPR(factor 40)\")\n",
    "plt.plot(range(epoch), train_loss_60, label=\"VBPR(factor 60)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top K rec test (해당 셀 이후부터는 아직 수정되지 않음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "class Recommender:\n",
    "    def __init__(self, model, query_img, train_dataset, n_item, img_emb, device) -> None:\n",
    "        self.model = model\n",
    "        self.train_df = train_dataset.dataset.df\n",
    "        self.all_item = set(range(0,n_item))\n",
    "        self.query_img = query_img\n",
    "        self.img_emb = img_emb\n",
    "        self.device = device\n",
    "\n",
    "    def _get_img_sim(self, itemset:list):\n",
    "        print(\"GET IMG SIM\")\n",
    "        res = []\n",
    "        for item in itemset:\n",
    "            res.append(nn.functional.cosine_similarity(self.query_img, self.img_emb[item.item()]))\n",
    "        return res\n",
    "\n",
    "    def _get_unobs_items(self, user_idx):\n",
    "        obs_item_set = set(self.train_df[self.train_df.user_id==user_idx].isbn)\n",
    "        return list(self.all_item - obs_item_set)\n",
    "\n",
    "    def user_rank(self, user_idx:int, top_k:int=None, img_sim_weight:float=0.5):\n",
    "        self.model.eval()\n",
    "        unobs_itemset = self._get_unobs_items(user_idx)\n",
    "        scaler = MaxAbsScaler()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            itemset = torch.tensor(unobs_itemset).to(self.device)\n",
    "            user = torch.tensor(np.full(len(itemset), user_idx)).to(self.device)\n",
    "            img_sim = torch.tensor(self._get_img_sim(itemset))\n",
    "\n",
    "            out, _ = self.model.cal_each(user, itemset)\n",
    "            out = scaler.fit_transform(out) # range [-1~1]\n",
    "            out = out + img_sim_weight*img_sim # range [-1.5~1.5]\n",
    "\n",
    "            scores = np.array(torch.concat((user.unsqueeze(dim=1),itemset.unsqueeze(dim=1),out.unsqueeze(dim=1)), dim=1))\n",
    "       \n",
    "        sorted_scores = scores[(-scores[:, 2]).argsort()]\n",
    "        return sorted_scores[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(recommender, test_dataset):\n",
    "    df = test_dataset.dataset.df\n",
    "    user_list = df['user_id'].unique()\n",
    "    res_true = {}\n",
    "    res_topk = {}\n",
    "    res_hit = {}\n",
    "    \n",
    "    for user in tqdm(df.iterrows(), total = len(df)):\n",
    "        res = recommender.user_rank(user, 20)\n",
    "        topk_item = res[:,1]\n",
    "        true_item = \n",
    "        hit = len(set(true_item).intersection(set(topk)))\n",
    "        res_true[user] = list(true_item)\n",
    "        res_topk[user] = list(topk)\n",
    "        res_hit[user] = hit\n",
    "    \n",
    "    return res_true, res_topk, res_hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = fclip.encode_images(query_img, batch_size=1)\n",
    "# res = img_sim(query, feat_map_vgg)\n",
    "# res\n",
    "recommender = Recommender(vbpr_20, query, train_dataset, n_item, img_emb, device)\n",
    "\n",
    "res_true, res_topk, res_hit = eval(recommender, test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sequential",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
