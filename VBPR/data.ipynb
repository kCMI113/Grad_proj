{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAKE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from huggingface_hub import HfApi\n",
    "from PIL import Image\n",
    "from tqdm import tqdm \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  \n",
    "seed = 42\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_dir(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        os.mkdir(file_path)\n",
    "    \n",
    "def get_timestamp(date_format: str = '%d%H%M%S') -> str:\n",
    "    timestamp = datetime.now()\n",
    "    return timestamp.strftime(date_format)\n",
    "\n",
    "data_dir = \"./data/\" + get_timestamp()\n",
    "mk_dir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_pickle(data, path):\n",
    "    with open(path, \"wb\") as file:\n",
    "        pickle.dump(data, file)\n",
    "\n",
    "def load_pickle(path):\n",
    "    with open(path, \"rb\") as file:\n",
    "        data = pickle.load(file)\n",
    "    return data\n",
    "\n",
    "def dump_json(data, path):\n",
    "    with open(path, \"w\") as file:\n",
    "        json.dump(data, file)\n",
    "\n",
    "def load_json(path):\n",
    "    with open(path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def save_pt(data, path):\n",
    "    with open(path, \"wb\") as file:\n",
    "        torch.save(data, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 load 및 shape 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_data = pd.read_csv(\"./data/articles.csv\")\n",
    "interaction_data = pd.read_csv(\"./data/transactions_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of Item data :  (105542, 25)\n",
      "shape of interaction data :  (31788324, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of Item data : \",item_data.shape)\n",
    "print(\"shape of interaction data : \", interaction_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 이미지가 없는 아이템 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_by_id(df, article_id:int, no_list:list, echo:int=1, img_show:bool=True):\n",
    "    if article_id in no_list:\n",
    "        return\n",
    "    if echo:\n",
    "        display(df[df.article_id == article_id])\n",
    "\n",
    "    img_id = \"0\"+str(article_id)\n",
    "    img = Image.open(\"./data/images/\"+img_id[0:3]+\"/\"+img_id+\".jpg\")\n",
    "\n",
    "    if img_show:\n",
    "        img.show()\n",
    "\n",
    "def find_no_img_item(df):\n",
    "    no_img = []\n",
    "\n",
    "    for item in tqdm(df.iterrows(), total=len(df)):\n",
    "        try:\n",
    "            img_by_id(df, item[1][0], no_list=no_img, echo=0, img_show=False)\n",
    "        except FileNotFoundError:\n",
    "            no_img.append(item[0])\n",
    "\n",
    "    return no_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_img_idx = find_no_img_item(item_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of non-img item :  442\n"
     ]
    }
   ],
   "source": [
    "print(\"# of non-img item : \",len(no_img_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of n_item_data :  (105100, 25)\n"
     ]
    }
   ],
   "source": [
    "# 이미지가 없는 아이템 삭제\n",
    "no_img_item = {idx:item_data.iloc[idx].article_id for idx in no_img_idx}\n",
    "n_item_data = item_data.drop(no_img_idx, axis=0).reset_index(drop=True) # 이미지 없는 아이템을 삭제한 데이터\n",
    "print(\"shape of n_item_data : \", n_item_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 상품 카테고리(product_type_no)에 따른 필터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of n_item_data :  (104973, 25)\n"
     ]
    }
   ],
   "source": [
    "# product_type에 속하는 상품이 10개 미만인 경우 삭제 131 -> 94로 줄어듦, 아이템은 약 130개 사라짐\n",
    "n_item_data = n_item_data.groupby('product_type_no').filter(lambda x: len(x) >= 10).reset_index(drop=True)\n",
    "print(\"shape of n_item_data : \", n_item_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of n_item_data :  (104572, 25)\n",
      "# of product_type :  84\n"
     ]
    }
   ],
   "source": [
    "# product_type 중 불필요한 것 삭제 -> 84로 줄어듧, 아이템 400개 정도 사라짐\n",
    "rm_list = [\"Umbrella\", \"Bracelet\", \"Giftbox\", \"Waterbottle\", \n",
    "           \"Nipple covers\", 'Chem. cosmetics', \"Fine cosmetics\", \"Soft Toys\",\n",
    "           \"Bra extender\", \"Cushion\", \"Side table\", \"Dog Wear\", \"Keychain\",\n",
    "           \"Sewing kit\", \"Towel\", \"Mobile case\", \"Zipper head\",\n",
    "           \"Wireless earphone case\", \"Stain remover spray\",\n",
    "           \"Clothing mist\", \"Hair ties\"]\n",
    "n_item_data = n_item_data[~n_item_data['product_type_name'].isin(rm_list)].reset_index(drop=True)\n",
    "print(\"shape of n_item_data : \", n_item_data.shape)\n",
    "print(\"# of product_type : \", n_item_data.product_type_name.nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of interaction data :  (31788324, 5)\n",
      "shape of n_interaction_data :  (31550289, 5)\n"
     ]
    }
   ],
   "source": [
    "# interacion data에서 앞선 과정에서 삭제된 데이터 제거\n",
    "n_interaction_data = interaction_data[interaction_data['article_id'].isin(n_item_data['article_id'])].reset_index(drop=True)\n",
    "print(\"shape of interaction data : \", interaction_data.shape)\n",
    "print(\"shape of n_interaction_data : \", n_interaction_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_item_data = n_item_data[[\"article_id\",\"product_type_no\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 아이템 및 유저의 상호작용 수에 따른 데이터 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아이템/유저 등장 빈도에 따른 데이터 구성\n",
    "# 유저: 상호작용이 threshold 이하인 경우 삭제, 아이템 : 상호작용이 20 이하인 경우 삭제 \n",
    "# 상호작용 유저/아이템 수는 반복 구매를 제외하고 unique한 값을 기준으로 함\n",
    "# 반복적으로 실행하여, 모든 유저, 아이템이 조건을 만족하도록 함\n",
    "\n",
    "def data_cutter(origin_data, threshold=20):\n",
    "    while True:\n",
    "        new_data = origin_data.groupby('customer_id').filter(lambda x: x.article_id.nunique() >= threshold).reset_index(drop=True)\n",
    "        new_data = new_data.groupby('article_id').filter(lambda x: x.customer_id.nunique() >= 20).reset_index(drop=True)\n",
    "        \n",
    "        if new_data.equals(origin_data):\n",
    "            print(\"finish\")\n",
    "            break\n",
    "        origin_data = new_data\n",
    "        print(\"cut again\")\n",
    "\n",
    "    print(\"shape of n_interaction_data : \", new_data.shape)\n",
    "    print(\"num of user : \", new_data.customer_id.nunique())\n",
    "    print(\"num of item : \", new_data.article_id.nunique())\n",
    "    print(\"data density : \", new_data.shape[0]/(new_data.customer_id.nunique()*new_data.article_id.nunique())*100, \"%\")\n",
    "    \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cut again\n",
      "cut again\n",
      "cut again\n",
      "cut again\n",
      "finish\n",
      "shape of n_interaction_data :  (17719441, 5)\n",
      "num of user :  190977\n",
      "num of item :  60286\n",
      "data density :  0.15390491199323902 %\n"
     ]
    }
   ],
   "source": [
    "# n_interaction_data_10 = data_cutter(n_interaction_data, 10)\n",
    "# n_interaction_data_20 = data_cutter(n_interaction_data, 20)\n",
    "new_interaction_data = data_cutter(n_interaction_data, 40) # 아이템: 20미만 삭제, 유저: 40미만 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\"shape of n_interaction_data\" : new_interaction_data.shape,\n",
    "            \"num of user\" : new_interaction_data.customer_id.nunique(),\n",
    "            \"num of item\" : new_interaction_data.article_id.nunique(),\n",
    "            \"data density\" : f'{new_interaction_data.shape[0]/(new_interaction_data.customer_id.nunique()*new_interaction_data.article_id.nunique())*100}%'}\n",
    "dump_json(metadata, f'{data_dir}/metadata.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# product_type마다 속하는 item 목록 생성 {product_type_no : [items]}\n",
    "items_by_prod_type = dict(list(n_item_data.groupby(\"product_type_no\")))\n",
    "for k in items_by_prod_type.keys():\n",
    "    items_by_prod_type[k] = items_by_prod_type[k].article_id.reset_index(drop=True).tolist()\n",
    "\n",
    "dump_pickle(items_by_prod_type, f'{data_dir}/items_by_prod_type_small.pkl')\n",
    "# dump_json(items_by_prod_type, f'{data_dir}/items_by_prod_type_small.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prod_type에 속하는 아이템이 5개 미만인 경우 prod_type과 그에 속하는 아이템 삭제\n",
    "rm_list = [key for key, value in items_by_prod_type.items() if len(value) < 5]\n",
    "\n",
    "for k in rm_list:\n",
    "    del items_by_prod_type[k]\n",
    "    \n",
    "new_itme_data = n_item_data[~n_item_data['product_type_no'].isin(rm_list)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_itme_data = n_item_data[n_item_data['article_id'].isin(new_interaction_data['article_id'])].reset_index(drop=True)\n",
    "new_interaction_data = new_interaction_data[new_interaction_data['article_id'].isin(n_item_data['article_id'])].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of inew_itme_data :  (60286, 2)\n",
      "shape of new_interaction_data :  (17719441, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of inew_itme_data : \", new_itme_data.shape)\n",
    "print(\"shape of new_interaction_data : \", new_interaction_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\"shape of interaction data\" : new_interaction_data.shape,\n",
    "            \"num of user\" : new_interaction_data.customer_id.nunique(),\n",
    "            \"num of item\" : new_interaction_data.article_id.nunique(),\n",
    "            \"data density\" : f'{new_interaction_data.shape[0]/(new_interaction_data.customer_id.nunique()*new_interaction_data.article_id.nunique())*100}%'}\n",
    "dump_json(metadata, f'{data_dir}/metadata.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 아이템 이미지 임베딩 생성 with fashion clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fashion_clip.fashion_clip import FashionCLIP\n",
    "fclip = FashionCLIP('fashion-clip')\n",
    "\n",
    "images = [\"./data/images/\" + \"0\" + str(k)[0:2] + \"/\" + \"0\"+str(k) + \".jpg\" for k in new_itme_data[\"article_id\"].tolist()]\n",
    "image_embeddings = fclip.encode_images(images, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {article_id : emb}\n",
    "id_emb_map = {k:torch.tensor(v) for k,v in zip(new_itme_data[\"article_id\"].tolist(), image_embeddings)}\n",
    "dump_pickle(id_emb_map, f'{data_dir}/id_emb_map.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이미지 임베딩을 pkl/json 파일로 저장하고 싶은 경우 실행\n",
    "dump_pickle(image_embeddings, f'{data_dir}/img_emb_small.pkl')\n",
    "# dump_json(image_embeddings, f'{data_dir}/img_emb_small.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### id mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of user 190977\n",
      "# of item 60286\n"
     ]
    }
   ],
   "source": [
    "user2idx = {v:k for k,v in enumerate(new_interaction_data['customer_id'].unique())} # {user_id:idx}\n",
    "item2idx = {v:k for k,v in enumerate(new_itme_data['article_id'].unique())}         # {item_id:idx}\n",
    "idx2item = {k:v for k,v in enumerate(new_itme_data['article_id'].unique())}         # {idx:item_id}\n",
    "\n",
    "print(\"# of user\", len(user2idx))\n",
    "print(\"# of item\", len(item2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_interaction_data[\"customer_id\"] = new_interaction_data[\"customer_id\"].map(user2idx)\n",
    "new_interaction_data[\"article_id\"] = new_interaction_data[\"article_id\"].map(item2idx)\n",
    "new_itme_data[\"article_id\"] = new_itme_data[\"article_id\"].map(item2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train/valid/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_interaction_data = new_interaction_data.drop([\"t_dat\", \"price\",\"sales_channel_id\"], axis=1)\n",
    "unique_data = new_interaction_data.drop_duplicates([\"article_id\", \"customer_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 및 valid가 중복없이 생성되었는가? : True\n"
     ]
    }
   ],
   "source": [
    "test_data = unique_data.groupby(\"customer_id\").sample(n=1, random_state=seed) # 랜덤하게 1개 추출하여 test로 사용\n",
    "unique_data = unique_data[~unique_data.index.isin(test_data.index)] # test set으로 추출된 것 제거\n",
    "valid_data = unique_data.groupby(\"customer_id\").sample(n=1, random_state=seed) # 다시 랜덤하게 1개 추출하여 valid로 사용\n",
    "\n",
    "print(f'test 및 valid가 중복없이 생성되었는가? : {pd.concat([valid_data, test_data]).drop_duplicates(keep=False).shape[0] == (valid_data.shape[0] + test_data.shape[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190977/190977 [01:24<00:00, 2259.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of test_data :  (190977, 2)\n",
      "shape of valid_data :  (190977, 2)\n",
      "shape of train_data :  (17273413, 2)\n"
     ]
    }
   ],
   "source": [
    "drop_index = pd.Index([None])\n",
    "\n",
    "for key, data in tqdm(new_interaction_data.groupby(\"customer_id\"), total=new_interaction_data.customer_id.nunique()):\n",
    "    valid_item = valid_data[valid_data.customer_id==key].article_id\n",
    "    test_item = test_data[test_data.customer_id==key].article_id\n",
    "    drop_index = drop_index.append(data[data.article_id.isin(np.concatenate((valid_item, test_item)))].index)\n",
    "\n",
    "drop_index = drop_index.dropna() # remove None\n",
    "train_data = new_interaction_data.drop(drop_index) \n",
    "\n",
    "test_data = test_data.reset_index(drop=True)\n",
    "valid_data = valid_data.reset_index(drop=True)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "print(\"shape of test_data : \", test_data.shape)\n",
    "print(\"shape of valid_data : \", valid_data.shape)\n",
    "print(\"shape of train_data : \", train_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>22733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>22734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>22731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  article_id\n",
       "0            0        3045\n",
       "1            0       22733\n",
       "2            0       22734\n",
       "3            0       22731\n",
       "4            0        3044"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>19032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>31981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>23996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>28524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  article_id\n",
       "0            0       19032\n",
       "1            1         785\n",
       "2            2       31981\n",
       "3            3       23996\n",
       "4            4       28524"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>52370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>52487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>10099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>26696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>58140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  article_id\n",
       "0            0       52370\n",
       "1            1       52487\n",
       "2            2       10099\n",
       "3            3       26696\n",
       "4            4       58140"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item 등장 빈도에 대한 목록 생성 {article_id : cnt}, trian_data에 있는 것만 반영\n",
    "# item_occur_cnt = train_data.groupby(\"article_id\").count().reset_index()\n",
    "# item_occur_cnt.rename(columns={\"customer_id\":\"cnt\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # product_type마다 속하는 item 목록 생성 {product_type_no : [[items], [cnts]]}\n",
    "# items_by_prod_type_cnt = dict(list(n_item_data.groupby(\"product_type_no\")))\n",
    "# for k in items_by_prod_type_cnt.keys():\n",
    "#     items_by_prod_type_cnt[k].rename(columns={\"product_type_no\":\"cnt\"}, inplace=True)\n",
    "#     items_by_prod_type_cnt[k] = items_by_prod_type_cnt[k].reset_index(drop=True)\n",
    "#     items_by_prod_type_cnt[k].cnt = 0\n",
    "\n",
    "#     for idx, row in items_by_prod_type_cnt[k].iterrows():\n",
    "#         if row.article_id in item_occur_cnt.article_id.values:\n",
    "#             items_by_prod_type_cnt[k].at[idx, 'cnt'] = item_occur_cnt.loc[item_occur_cnt['article_id'] == row.article_id, 'cnt'].values[0]\n",
    "\n",
    "# dump_pickle(items_by_prod_type_cnt, \"items_by_prod_type_cnt.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user의 pos item 목록 생성 {user_id : [items]}\n",
    "pos_items_each_user = dict(list(train_data.groupby(\"customer_id\")))\n",
    "for k in pos_items_each_user.keys():\n",
    "    pos_items_each_user[k] = pos_items_each_user[k].article_id.reset_index(drop=True).tolist()\n",
    "\n",
    "dump_pickle(pos_items_each_user, f'{data_dir}/pos_items_each_user_small.pkl')\n",
    "# dump_json(pos_items_each_user, f'{data_dir}/pos_items_each_user_small.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATASET 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMTrainDataset(Dataset):\n",
    "    def __init__(self, df, item_df, items_by_prod_type, pos_items_each_user) -> None:\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.item_df = item_df\n",
    "        self.items_by_prod_type = items_by_prod_type\n",
    "        self.pos_items_each_user = pos_items_each_user\n",
    "        self.df['neg'] = np.zeros(len(self.df), dtype=int)\n",
    "        self._make_triples_data()\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        user = self.df.iloc[index]['customer_id']\n",
    "        pos = self.df.iloc[index]['article_id']\n",
    "        neg = self.df.iloc[index]['neg']\n",
    "        return user, pos, neg\n",
    "    \n",
    "    def _neg_sampling(self, pos_list, prod_type_no):\n",
    "        # 같은 prod_type_no 내에서 neg sampling\n",
    "        neg = random.choice(self.items_by_prod_type[prod_type_no]) \n",
    "        while neg in pos_list:\n",
    "            neg = random.choice(self.items_by_prod_type[prod_type_no]) \n",
    "        return neg\n",
    "\n",
    "    def _make_triples_data(self):\n",
    "        for user_id, rows in tqdm(self.df.groupby(\"customer_id\")):\n",
    "            pos_list = self.pos_items_each_user[user_id]\n",
    "            for idx, row in rows.iterrows():\n",
    "                item_id = row.article_id\n",
    "                prod_type_no = self.item_df[self.item_df[\"article_id\"] == item_id].product_type_no.item()\n",
    "                self.df.at[idx, 'neg'] = self._neg_sampling(pos_list, prod_type_no)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMDataset(Dataset):\n",
    "    def __init__(self, df) -> None:\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        user = self.df.iloc[index]['customer_id']\n",
    "        pos = self.df.iloc[index]['article_id']\n",
    "        return user, pos\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190977/190977 [29:14<00:00, 108.86it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = HMTrainDataset(train_data, new_itme_data, items_by_prod_type, pos_items_each_user)\n",
    "save_pt(train_dataset, f'{data_dir}/train_dataset_small.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = HMDataset(valid_data)\n",
    "save_pt(valid_dataset, f'{data_dir}/valid_dataset_small.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = HMDataset(test_data)\n",
    "save_pt(test_dataset, f'{data_dir}/test_dataset_small.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### candidate item set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_items = np.arange(len(item2idx))\n",
    "sample_size = 1000\n",
    "candidate_items_each_user = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190977/190977 [01:48<00:00, 1759.79it/s]\n"
     ]
    }
   ],
   "source": [
    "for user, target in tqdm(test_dataset):\n",
    "    candidate_items = torch.tensor(np.append(np.random.choice(np.setdiff1d(all_items, pos_items_each_user[user]), sample_size), target))\n",
    "    candidate_items_each_user[user] = candidate_items\n",
    "\n",
    "dump_pickle(candidate_items_each_user, f'{data_dir}/candidate_items_each_user_small.pkl')\n",
    "# dump_json(candidate_items_each_user, f'{data_dir}/candidate_items_each_user_small.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### upload data to Huggingface Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/20182752'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login --token \"your write token\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to Huggingface Hub\n",
    "api = HfApi()\n",
    "api.upload_folder(folder_path=data_dir, path_in_repo=f\"{data_dir[-8:]}\", repo_id=\"SLKpnu/HandM_Dataset\", \n",
    "                  commit_message=f\"dataset created timestamp : {data_dir[-8:]}\", repo_type=\"dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 실행 시간 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.random.randint 실행 시간: 6.5607867789804 초\n",
      "random.sample 실행 시간: 0.37414733300101943 초\n",
      "random.choice 실행 시간: 0.17139070699340664 초\n",
      "np.random.choice 실행 시간: 13.791083243995672 초\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def function_1(pos_list, n_item):\n",
    "    neg = np.random.randint(0, n_item, 1) \n",
    "    while neg in pos_list:\n",
    "        neg = np.random.randint(0, n_item, 1) \n",
    "    return neg\n",
    "\n",
    "def function_2(pos_list, n_item):\n",
    "    neg = random.sample(range(0,n_item), 1) \n",
    "    while neg in pos_list:\n",
    "        neg = random.sample(range(0,n_item), 1) \n",
    "    return neg\n",
    "\n",
    "def function_3(pos_list, n_item):\n",
    "    neg = random.choice(range(0,n_item)) \n",
    "    while neg in pos_list:\n",
    "        neg = random.choice(range(0,n_item)) \n",
    "    return neg\n",
    "\n",
    "def function_4(pos_list, n_item):\n",
    "    neg = np.random.choice(np.arange(n_item), 1)\n",
    "    while neg in pos_list:\n",
    "        neg =  np.random.choice(np.arange(n_item), 1)\n",
    "    return neg\n",
    "\n",
    "n_item = len(item2idx)\n",
    "pos = random.sample(range(1,n_item), 40)\n",
    "n = 115500\n",
    "\n",
    "time_1 = timeit.timeit('function_1(pos, n_item)', globals=globals(), number=n)\n",
    "time_2 = timeit.timeit('function_2(pos, n_item)', globals=globals(), number=n)\n",
    "time_3 = timeit.timeit('function_3(pos, n_item)', globals=globals(), number=n)\n",
    "time_4 = timeit.timeit('function_4(pos, n_item)', globals=globals(), number=n)\n",
    "\n",
    "print(f'np.random.randint 실행 시간: {time_1} 초')\n",
    "print(f'random.sample 실행 시간: {time_2} 초')\n",
    "print(f'random.choice 실행 시간: {time_3} 초')\n",
    "print(f'np.random.choice 실행 시간: {time_4} 초')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(pos_list, n_item):\n",
    "    neg = random.choice(range(0,n_item)) \n",
    "    while neg in pos_list:\n",
    "        neg = random.choice(range(0,n_item)) \n",
    "    return neg\n",
    "\n",
    "def function_2(pos_list, n_item):\n",
    "    pos_list = {k:1 for k in pos_list}\n",
    "    neg = random.choice(range(0,n_item)) \n",
    "    while neg in pos_list:\n",
    "        neg = random.choice(range(0,n_item)) \n",
    "    return neg\n",
    "\n",
    "n_item = len(item2idx)\n",
    "pos = random.sample(range(1,n_item), 40)\n",
    "n = 315500\n",
    "\n",
    "time_1 = timeit.timeit('function_1(pos, n_item)', globals=globals(), number=n)\n",
    "time_2 = timeit.timeit('function_2(pos, n_item)', globals=globals(), number=n)\n",
    "\n",
    "print(f'list 실행 시간: {time_1} 초')\n",
    "print(f'dict 실행 시간: {time_2} 초')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list(set) 실행 시간: 508.1635248339999 초\n",
      "np.setdiff1d 실행 시간: 69.18620454200027 초\n",
      "set 실행 시간: 5.8264562089998435 초\n",
      "np.arange 실행 시간: 0.028278708000470942 초\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import timeit\n",
    "import random\n",
    "import torch\n",
    "\n",
    "def function_1(pos_list, all_item):\n",
    "    candicate_items = torch.tensor(list(all_item - set(pos_list)))\n",
    "\n",
    "def function_2(pos_list, all_item):\n",
    "    candicate_items = torch.tensor(np.setdiff1d(all_item, pos_list))\n",
    "   \n",
    "def function_3():\n",
    "    items_set = set(range(1,104573))\n",
    "    \n",
    "def function_4():\n",
    "    items_np = np.arange(104573, dtype=np.int32)\n",
    "    \n",
    "\n",
    "items_set = set(range(1,104573))\n",
    "items_np = np.arange(104573, dtype=np.int32)\n",
    "pos = random.sample(range(1,104573), 40)\n",
    "n1 = 74570\n",
    "n2 = 4000\n",
    "\n",
    "time_1 = timeit.timeit('function_1(pos, items_set)', globals=globals(), number=n1)\n",
    "print(f'list(set) 실행 시간: {time_1} 초')\n",
    "\n",
    "time_2 = timeit.timeit('function_2(pos, items_np)', globals=globals(), number=n1)\n",
    "print(f'np.setdiff1d 실행 시간: {time_2} 초')\n",
    "\n",
    "time_3 = timeit.timeit('function_3()', globals=globals(), number=n2)\n",
    "print(f'set 실행 시간: {time_3} 초')\n",
    "\n",
    "time_4 = timeit.timeit('function_4()', globals=globals(), number=n2)\n",
    "print(f'np.arange 실행 시간: {time_4} 초')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.random.choice 실행 시간: 41.994027569977334 초\n",
      "random.sample + list 실행 시간: 174.8324626859976 초\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import timeit\n",
    "import random\n",
    "import torch\n",
    "\n",
    "def function_1(pos_list, all_item):\n",
    "    candicate_items = torch.tensor(np.random.choice(np.setdiff1d(all_item, pos_list), 100))\n",
    "\n",
    "\n",
    "def function_2(pos_list, all_item):\n",
    "    candicate_items = torch.tensor(random.sample(list(np.setdiff1d(all_item, pos_list)), 100))\n",
    "    \n",
    "\n",
    "# items_set = set(range(1,104573))\n",
    "items_np = np.arange(104573, dtype=np.int32)\n",
    "pos = random.sample(range(1,104573), 40)\n",
    "n1 = 24570\n",
    "\n",
    "time_1 = timeit.timeit('function_1(pos, items_np)', globals=globals(), number=n1)\n",
    "print(f'np.random.choice 실행 시간: {time_1} 초')\n",
    "\n",
    "time_2 = timeit.timeit('function_2(pos, items_np)', globals=globals(), number=n1)\n",
    "print(f'random.sample + list 실행 시간: {time_2} 초')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vbpr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
